<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[烦闷的奔波]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-29%2F</url>
    <content type="text"><![CDATA[你总是疲惫地仰望天空，然而，你却看不到未来的银河。 这几天为自己未来操心操力，时紧时松，也是够了。 不知道该碎碎语什么，逐渐发觉自己对世界的认知太过单纯，单纯地以为只有一颗理想主义的心就能获得理想主义的青睐，然后可以坚持原则，没用实质的东西跟别人表示我很行，并不意味着我不行啊，然而世界的话语体系是建立在“看得见”的事实之上，没有人真会耐心地等待你的努力，没有人会真地等你发挥你的潜力，所有成绩都是靠象征性的数字或者纸张来表达的。如果你没有纸张，你就没有成绩；如果你没有数字，你就没有成果。人的存在束缚在了这些零零碎碎的体系之中，然后我们以此为生，世世代代。 这就是现实啊，我是不是该摇醒我自己，看看周围，看看镜子中的自己。]]></content>
      <categories>
        <category>生活日志</category>
      </categories>
      <tags>
        <tag>思考</tag>
        <tag>人生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何利用 Python 抓取 Twitter 的内容]]></title>
    <url>%2F2017%2F10%2F20%2F2017-10-20-1%2F</url>
    <content type="text"><![CDATA[这篇总结一下初次使用 python 抓取推文的学习心得吧，其实也就是跑了下书中的代码。技术流程就是前文展示的那种，那么这里就按照这个流程来一遍吧。（本部分涉及到书中的 Chap2-3 ） 验证登入 Twitter App 获取 key1、进入网站2、新建应用3、获取以下 key 值consumer key， consumer secret， access token， access secret 运行通用型认证代码在指定位置填入刚才获得的 key1234567891011121314151617181920212223242526272829303132333435# 用于获得推特的准入许可import osimport sysfrom tweepy import APIfrom tweepy import OAuthHandlerdef get_twitter_auth(): """启动 Twitter 验证 返回: tweepy.OAuthHandler 对象 """ try: # 下面的代码不知道如何解决，设置了环境变量，但是依然有错误，所以采用直接形式 #consumer_key = os.environ['TWITTER_CONSUMER_KEY'] #consumer_secret = os.environ['TWITTER_CONSUMER_SECRET'] #access_token = os.environ['TWITTER_ACCESS_TOKEN'] #access_secret = os.environ['TWITTER_ACCESS_SECRET'] consumer_key = '自己填' consumer_secret = '自己填' access_token = '自己填' access_secret = '自己填' except KeyError: sys.stderr.write("TWITTER_* environment variables not set\n") sys.exit(1) auth = OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_secret) return authdef get_twitter_client(): """启动 Twitter API 客户端 返回: tweepy.API 对象 """ auth = get_twitter_auth() # 这里要注意设置代理的网址 client = API(auth,proxy="自己填") return client 这样我们就获得了进入 Twitter 的许可证了，如书中这样把这块代码作为单独的文件，命名为 twitter_client.py 数据收集许可证程序已经做好了，接下来是爬取数据，书中介绍了几种数据内容，安装处理数据的方式分为：文本内容、时间序列、关系和地理。但也可以理解为：静态数据和动态数据。静态数据主要包括：本人关注信息流、指定用户发布信息、指定用户粉丝和关注信息；动态数据主要是指定标签和内容的信息流（这里就涉及到不同用户的发布信息） 静态数据 REST API本人主页关注信息流：twitter_get_home_timeline.py12345678910111213# 用于获取本人关注的信息流import jsonfrom tweepy import Cursorfrom twitter_client import get_twitter_clientif __name__ == '__main__': client = get_twitter_client() with open('home_timeline.jsonl', 'w') as f: for page in Cursor(client.home_timeline, count=200, include_rts=True).pages(4): for status in page: # Process a single status f.write(json.dumps(status._json)+"\n") 指定用户发布信息流：twitter_get_user_timeline.py1234567891011121314151617181920212223# 获取指定用户的信息流import sysimport jsonfrom tweepy import Cursorfrom twitter_client import get_twitter_clientdef usage(): print("Usage:") print("python &#123;&#125; &lt;username&gt;".format(sys.argv[0]))if __name__ == '__main__': if len(sys.argv) != 2: usage() sys.exit(1) user = sys.argv[1] client = get_twitter_client() fname = "user_timeline_%s.jsonl" % user with open(fname, 'w') as f: # 设定到了最大值：3200条 for page in Cursor(client.user_timeline, screen_name=user, count=200).pages(16): for status in page: f.write(json.dumps(status._json)+"\n") 指定用户粉丝信息和关注信息：twitter_get_user.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 获取指定用户的粉丝和关注信息import osimport sysimport jsonimport timeimport mathfrom tweepy import Cursorfrom twitter_client import get_twitter_client# 设定了最大值MAX_FRIENDS = 15000def usage(): print("Usage:") print("python &#123;&#125; &lt;username&gt;".format(sys.argv[0]))def paginate(items, n): """Generate n-sized chunks from items""" for i in range(0, len(items), n): yield items[i:i+n]if __name__ == '__main__': if len(sys.argv) != 2: usage() sys.exit(1) screen_name = sys.argv[1] client = get_twitter_client() dirname = "users/&#123;&#125;".format(screen_name) max_pages = math.ceil(MAX_FRIENDS / 5000) try: os.makedirs(dirname, mode=0o755, exist_ok=True) except OSError: print("Directory &#123;&#125; already exists".format(dirname)) except Exception as e: print("Error while creating directory &#123;&#125;".format(dirname)) print(e) sys.exit(1) # 获得指定用户的粉丝信息 fname = "users/&#123;&#125;/followers.jsonl".format(screen_name) with open(fname, 'w') as f: for followers in Cursor(client.followers_ids, screen_name=screen_name).pages(max_pages): for chunk in paginate(followers, 100): users = client.lookup_users(user_ids=chunk) for user in users: f.write(json.dumps(user._json)+"\n") if len(followers) == 5000: print("More results available. Sleeping for 60 seconds to avoid rate limit") time.sleep(60) # 获取指定用户的关注信息 fname = "users/&#123;&#125;/friends.jsonl".format(screen_name) with open(fname, 'w') as f: for friends in Cursor(client.friends_ids, screen_name=screen_name).pages(max_pages): for chunk in paginate(friends, 100): users = client.lookup_users(user_ids=chunk) for user in users: f.write(json.dumps(user._json)+"\n") if len(friends) == 5000: print("More results available. Sleeping for 60 seconds to avoid rate limit") time.sleep(60) # 获取用户信息 fname = "users/&#123;&#125;/user_profile.json".format(screen_name) with open(fname, 'w') as f: profile = client.get_user(screen_name=screen_name) f.write(json.dumps(profile._json, indent=4)) 指定内容的检索信息动态数据 Streaming API指定标签和内容的信息流：twitter_streaming.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 获取指定标签和内容的信息流import sysimport stringimport timefrom tweepy import Streamfrom tweepy.streaming import StreamListenerfrom twitter_client import get_twitter_authclass CustomListener(StreamListener): """Custom StreamListener for streaming Twitter data.""" def __init__(self, fname): safe_fname = format_filename(fname) self.outfile = "stream_%s.jsonl" % safe_fname def on_data(self, data): try: with open(self.outfile, 'a') as f: f.write(data) return True except BaseException as e: sys.stderr.write("Error on_data: &#123;&#125;\n".format(e)) time.sleep(5) return True def on_error(self, status): if status == 420: sys.stderr.write("Rate limit exceeded\n".format(status)) return False else: sys.stderr.write("Error &#123;&#125;\n".format(status)) return Truedef format_filename(fname): """Convert fname into a safe string for a file name. Return: string """ return ''.join(convert_valid(one_char) for one_char in fname)def convert_valid(one_char): """Convert a character into '_' if "invalid". Return: string """ valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits) if one_char in valid_chars: return one_char else: return '_'if __name__ == '__main__': query = sys.argv[1:] # list of CLI arguments query_fname = ' '.join(query) # string auth = get_twitter_auth() twitter_stream = Stream(auth, CustomListener(query_fname)) twitter_stream.filter(track=query, async=True) 数据清理和预处理建模和分析结果展示get_twitter_auth() 获得许可证get_twitter_client() 登入服务器 小实验：获取自己主页时间线的前十条tweepy.Cursortweepy.Status 小实验2: 获取主页200条并存入 json自己主页最多 800条；别人主页最多 3200条； tweet 的数据结构streaming APICustomListener class StreamListener on_data() on_error()#19thCPC #19thPartyCongress 数据分析标签频率；推文标签分布；提及人频率； 文本分析Tokenization 分词stop word removal 停顿词 时间序列分析粉丝状况 在量很大的情况下，选择 set 或者 numpy 来计算； 测量影响力Term Frequency (TF)Inverse Document Frequency (IDF)TF-IDF statistics 评论关系未完全检测到，可能跟数据有关 地图绘制]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么理解 Python 爬虫]]></title>
    <url>%2F2017%2F10%2F20%2F2017-10-20%2F</url>
    <content type="text"><![CDATA[由于自己的研究需要爬取 Twitter 和 Facebook 的社交数据，但自己又没有真正爬取过，那么我就来好好学习一下啦。 选择一本书我选择了《Mastering social media mining with Python》这本书，里面的讲解还是很直接的，干货多方便我快速掌握。本书的代码都能在网站上获取 了解一些理论第一章是 Social media - challenges and opportunities ，讲的是一些爬虫的基本知识，还有一些技术展示，重在培养对爬虫的兴趣。 对社交媒体的理解Internet-based applicationsUser-generated contentNetworking 本书要解决的问题how to extract useful knowledge from the data coming from the social media? knowledge hierarchy 社交媒体挖掘的机遇Application Programming Interface (API)Representational State Transfer (REST)RESTful API 社交媒体挖掘的挑战big datastructured dataunstructured data semi-structured data data integrity data access research and development (R&amp;D) processes 社交媒体挖掘的技术流程1、验证 Authentication2、数据收集 Data collection3、数据清理和预处理 Data cleaning and pre-processing4、建模和分析 Modeling and analysis5、结果展示 Result presentation Open Authorization (OAuth)a user, consumer (our application), and resource provider (the social media platform). Text miningGraph mining Python tools for data sciencePython 的优点Declarative and intuitive syntaxRich ecosystem for data processingEfficiency Python 版本3.4+ and 3.5+pip and virtualenvvirtualenv 管理虚拟环境和安装依赖环境Conda, Anaconda, and Miniconda 数据分析工具NumPy and pandasNaming conventionsnumpy as np; pandas as pd; 机器学习Supervised learningNaive Bayes (NB)Support Vector Machine (SVM)Neural Networks (NN)training datatest dataUnsupervised learning]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终于弄好了～]]></title>
    <url>%2F2017%2F10%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[花了一点时间弄这个，可以建一个自己的博客也是很好了。]]></content>
      <categories>
        <category>生活日志</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
