<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Journal of Communication, Volume 67, Issue 1]]></title>
    <url>%2F2017%2F10%2F30%2F2017-10-30%2F</url>
    <content type="text"><![CDATA[本期 Journal of Communication 主要关注了冲突报道、网络隐私问题、政治新闻报道、父母对孩子使用互联网的介入、媒介可供性以及公民的政治讨论等议题。 融合新闻？以巴冲突在国内和全球报道上异同的纵向研究Convergent News? A Longitudinal Study of Similarity and Dissimilarity in the Domestic and Global Coverage of the Israeli-Palestinian Conflict Baden, C., &amp; Tenenboim-Weinblatt, K. (2017). Convergent News? A Longitudinal Study of Similarity and Dissimilarity in the Domestic and Global Coverage of the Israeli-Palestinian Conflict. Journal of Communication, 67(1), 1–25. 同一事件的新闻报道同时受到同质和异质因素的影响。在这篇论文中，我们通过分析在13个以色列、巴勒斯坦和国际主流媒体中超过十年的以巴冲突报道，研究了在不同媒体中的冲突报道是否以及什么时候变得越来越相似或者不同。我们区分了新闻中持续相似、逐渐融合和短暂离合的动因，并将它们与20多万新闻文本中明显的概念关系模式相关联。我们发现新闻中存在一种缓慢的、语境依赖的融合趋势，以及在有关重大冲突事件的解释中的暂时联合和分离。本研究探讨了潜在的交互作用，并指出研究当前全球新闻业变革的意义。 网络隐私顾虑和隐私管理：一个文献荟萃分析Online Privacy Concerns and Privacy Management: A Meta-Analytical Review Baruh, L., Secinti, E., &amp; Cemalcilar, Z. (2017). Online Privacy Concerns and Privacy Management: A Meta-Analytical Review. Journal of Communication, 67(1), 26–53. 本文使用了荟萃分析方法研究了个人的“隐私顾虑”和“隐私素养”能否预测人们网络服务和社交网站的使用、人们是否分享信息和是否采纳隐私保护措施。分析选用了来自34个国家（n = 75,269）的共计166项研究。隐私顾虑并不能预测社交网站的使用情况，这符合隐私悖论（privacy paradox）的假设。但是关心隐私的用户几乎不会使用在线服务以及分享信息，他们更可能运用隐私保护措施。除了信息分享，他们在意图和行为上也存在一致性。本研究也认为隐私素养在提高隐私保护措施使用能力上有重要作用。研究发现适用于不同性别、文化导向和国家法律体系。 Meta 分析是一种将多项研究结果进行定量合成分析的统计学方法，始于20世纪70~80年代，最初被定义为“收集大量单项试验进行结果整合的统计学分析”；1991年，Fleiss 提出了较严谨和准确的定义，“Meta 分析是用于比较和综合针对同一科学问题研究结果的统计学方法，其结论是否有意义取决于纳入研究的质量”。这说明并非所有 Meta 分析都能得出高质量结果和结论，只有对纳入研究进行同质性检验，分析异质性的原因，按同质性因素进行合并的 Meta 分析才可能有意义。仅纳入随机对照试验（RCT）的 Meta 分析得出的结果一般偏倚较小，其结论准确性远比单项试验高。 选择严肃还是讽刺，支持还是激动人心的新闻？政党的选择性接触与嘲讽新闻网络视频Selecting Serious or Satirical, Supporting or Stirring News? Selective Exposure to Partisan versus Mockery News Online Videos Knobloch-Westerwick, S., &amp; Lavis, S. M. (2017). Selecting Serious or Satirical, Supporting or Stirring News? Selective Exposure to Partisan versus Mockery News Online Videos. Journal of Communication, 67(1), 54–81. 本研究将认知不协调理论与娱乐-教育体系（entertainment-education frameworks）相结合来分析新闻的选择性和效果。我们用网络视频来检测对讽刺和政党报道的选择性接触，并以此检验有关克服反抗说服性信息的研究假设。实验（n=146）显示新闻的选择在立场（保守 vs. 自由）和题材（严肃政党报道 vs. 讽刺报道）上有所不同。研究结果显示出政治利益培植了严肃政党报道的选择。与政党一致的视频更常被选择；只有在讽刺报道视频面前，民主党并不展现如此稳固的偏见。选择的讽刺报道影响了内部的政治效果，并且选择的在线视频依据信息的立场引起了态度的强化。 最大化儿童的互联网使用机会和最小化面临的风险：父母介入的新兴策略中数字技能的作用Maximizing Opportunities and Minimizing Risks for Children Online: The Role of Digital Skills in Emerging Strategies of Parental Mediation Livingstone, S., Ólafsson, K., Helsper, E. J., Lupiáñez-Villanueva, F., Veltri, G. A., &amp; Folkvord, F. (2017). Maximizing Opportunities and Minimizing Risks for Children Online: The Role of Digital Skills in Emerging Strategies of Parental Mediation. Journal of Communication, 67(1), 82–105. 随着互联网在家庭中的广泛使用，父母不仅尝试最大化它们孩子使用互联网的机会，也在尽可能降低互联网使用的风险。我们调查了8个欧洲国家中6-14岁孩子的父母（N=6,400）。因子分析揭示了2个父母介入策略。介入的实施与提高互联网使用机会有关，但要面临存在的风险。这个策略包括了安全措施，对儿童机构的反应，以及在父母或孩子有一定数字技能之时的应用，因此这个策略可能不会造成危害。有限介入与面对互联网更少的风险有关，但要以互联网使用机会为代价，这再次审视了把媒介使用作为主要问题的政策建议。本研究证实了当父母或孩子数字技能都较低的时候，有可能保护脆弱孩子的安全，然而也会不利于他们学习数字技能。 组织性媒介可供性：操作化和媒介使用的关联Organizational Media Affordances: Operationalization and Associations with Media Use Rice, R. E., Evans, S. K., Pearce, K. E., Sivunen, A., Vitak, J., &amp; Treem, J. W. (2017). Organizational Media Affordances: Operationalization and Associations with Media Use. Journal of Communication, 67(1), 106–130. 越来越多研究将可供性这个概念用于分析组织语境下的信息和传播技术（ICTs）。然而，几乎没有研究对可供性进行操作化，以限定比较和程序化的研究。本文简要地从一般的和媒体的角度来回顾了可供性的概念化和可能情况，然后介绍了作为组织资源的组织媒体可供性概念。本研究利用来自一家北欧大型媒体组织的问卷调查数据，识别了六个可靠且有效的组织媒体可供性：遍在、可编辑、自我呈现、可检索、可访问和意识。基于在三个组织层级之一的十种媒体使用频率的八级媒介量表与这些可供性各有不同。来自这项研究的概念化、测量方法和结果能够为今后的组织传播和传播技术研究提供参考。 什么时候我们停止讨论政治：争议时期的对话维护和关闭When We Stop Talking Politics: The Maintenance and Closing of Conversation in Contentious Times Wells, C., Cramer, K. J., Wagner, M. W., Alvarez, G., Friedland, L. A., Shah, D. V., … Franklin, C. (2017). When We Stop Talking Politics: The Maintenance and Closing of Conversation in Contentious Times. Journal of Communication, 67(1), 131–157. 尽管公民讨论政治对民主而言有很重要的意义，传播学的研究并未考虑到这种讨论该怎样承受住来自我们公民文化的压力。我们分析了在一个典型的政治争议案例期间的政治讨论：2012年威斯康星州州长 Scott Walker 的罢免。结合了定性和定量的方法，我们发现公民文化分歧产生于许多市民觉得政治讨论无法继续的情况中。个体在错误的争端阵营中，受到职业性质、地理位置和其他个人环境因素的影响，更倾向于分裂。我们的研究结果质疑了在民主功能性的影响中，政治讨论能够在多极化和碎片化的时期为政治和社会差异之间搭建起沟通桥梁的能力。]]></content>
      <categories>
        <category>新传顶级期刊概览</category>
      </categories>
      <tags>
        <tag>摘要翻译</tag>
        <tag>jcom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[烦闷的奔波]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-29%2F</url>
    <content type="text"><![CDATA[你总是疲惫地仰望天空，然而，你却看不到未来的银河。 这几天为自己未来操心操力，时紧时松，也是够了。 不知道该碎碎语什么，逐渐发觉自己对世界的认知太过单纯，单纯地以为只有一颗理想主义的心就能获得理想主义的青睐，然后可以坚持原则，没用实质的东西跟别人表示我很行，并不意味着我不行啊，然而世界的话语体系是建立在“看得见”的事实之上，没有人真会耐心地等待你的努力，没有人会真地等你发挥你的潜力，所有成绩都是靠象征性的数字或者纸张来表达的。如果你没有纸张，你就没有成绩；如果你没有数字，你就没有成果。人的存在束缚在了这些零零碎碎的体系之中，然后我们以此为生，世世代代。 这就是现实啊，我是不是该摇醒我自己，看看周围，看看镜子中的自己。]]></content>
      <categories>
        <category>生活日志</category>
      </categories>
      <tags>
        <tag>思考</tag>
        <tag>人生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何利用 Python 抓取 Twitter 的内容]]></title>
    <url>%2F2017%2F10%2F20%2F2017-10-20-1%2F</url>
    <content type="text"><![CDATA[这篇总结一下初次使用 python 抓取推文的学习心得吧，其实也就是跑了下书中的代码。技术流程就是前文展示的那种，那么这里就按照这个流程来一遍吧。（本部分涉及到书中的 Chap2-3 ）我们利用 Python 中的 Tweepy 模块来进行 Twitter 抓取。本部分使用的编辑器是：Jupyter Notebook【需要添加安装教程】使用的 Python 版本：3.6.4 or 3.0+主要用的模块：Tweepy Twitter 许可验证创建 Twitter App 获取认证所需信息【需要添加认证方法教程】1、进入网站 Twitter Application Management 。2、点击 Creat New App 新建应用。3、新建完之后，点开刚建的 App，在 Keys and Access Tokens 页面中记录以下项目的值：Consumer Key ， Consumer Secret ， Access Token ， Access Token Secret。4、注意 Consumer Key 和 Consumer Secret 还分别是 API Key 和 API Secret 。 获得 Twitter 认证Twitter 的认证有两种方法，一种是用户自己登入进去的 User authentication ( OAuthHandler ) [1] ；另一种是以 APP 方式登入进去的 Application-only authentication ( AppAuthHandler )。[2] 两种方法在使用权限上都有限制，通常我们爬取 Twitter ，感兴趣的是其他用户的内容而非自己的，所以爬取主要是针对指定用户或者是指定关键词的相关内容。在调用 Search API 爬取方面，Application-only authentication 的认证模式更为宽松，每秒可以获得 450 下请求，以每次请求获取最多 100 条推文来计算，每 15 分钟可以获得 45,000 条推文，这大概是前一种模式的 2.5 倍。[3] 但是，在某些方面 User authentication 也更有优势，[4] 这就要根据自己研究需要进行切换了，这里我们采用 Application-only authentication 的认证模式。 1234567891011121314151617181920212223242526272829303132# 导入用于获得推特准入许可的模块from tweepy import APIfrom tweepy import OAuthHandler# 定义两个函数# 第一个是存入认证方式def get_twitter_auth(): consumer_key = '自己填' consumer_secret = '自己填' access_token = '自己填' access_secret = '自己填'# 若采用 User 的模式，则删除前面的“#”，并将之填在 APP 模式前 #auth = OAuthHandler(consumer_key, consumer_secret) #auth.set_access_token(access_token, access_secret)# 采用 APP 认证方式，提高可获取量 auth = AppAuthHandler(consumer_key, consumer_secret) return auth# 第二个是与客户端通信def get_twitter_client(): auth = get_twitter_auth()# 这里要注意设置代理的网址，因为科学上网的缘故，“proxy”填入代理ip client = API(auth,proxy="自己填", wait_on_rate_limit=True, wait_on_rate_limit_notify=True) if (not API): print ("无法认证") sys.exit(-1) return client 简单测试前面的准备工作做好之后，我们可以通过运行一小段代码来检验是否连通： 123client = get_twitter_client()for status in Cursor(client.search, q='china').items(10): print(status.text) 如果成功连接了，这段代码会输出 10 条包含 “china” 的推文文本内容，比如下面这样的： RT @yicaichina: #China’s #Hebei Province, #LaureateScienceAlliance Discuss Setting Up Lab for #NobelWinners in #XionganNewArea https://t.co… RT @JackCanalha: Lugares mais populosos do mundo: China EUA Índia Minha casa de mosquitos RT @d_QfbKzDTe17: China van die aggressor nasie is indringer Suidsee omgewing sand. 侵略国家のシナは南沙海域を侵略しています。アフリカーンス語 https://t.co/MThj4abonl @_____china_rock あと4㌢はきついから削れて？笑 RT @yicaichina: #BAIC Motor Subsidiary to Build 3,000 #NEV Battery Swapping Stations in #China https://t.co/IJjCyEY3TQ https://t.co/qyImsHA… China van de agressor natie is zuiden zand Zeegebied invasie. 侵略国家のシナは南沙海域を侵略しています。オランダ語 https://t.co/I3yC40kynF RT @yicaichina: #BAIC Motor Subsidiary to Build 3,000 #NEV Battery Swapping Stations in #China https://t.co/IJjCyEY3TQ https://t.co/qyImsHA… RT @yicaichina: #China’s Industrial Internet Platform #CASICloud Plans to Introduce New #Investors https://t.co/k3mW7S4I0b https://t.co/Sss… RT @LotusCreekKR: [이게 중국의 수준이다] 국가(國歌) 에 대한 모독죄가 3년 징역형이라는 나라. 우리나라 공식행사에서 애국가 대신 다른 노래 부르는 분들 중에 친중파 참 많은데..... https://t.co/pyCzFupxSg Safe Flight @teacheryanyan wala akong Lunch buddy ng 4 days. Ingat kayo ni Tom and your sibs sa China! Lakas lang maka-conyo ng “sibs” 数据抓取许可证程序已经做好了，接下来是爬取数据。从时间的角度，我们可以将数据分为静态数据，也就是已经存在的数据，或者说历史数据；还有一种是动态数据，就是实时跟踪推文的发布，同步抓取。受到网络环境的影响，动态数据的抓取难以测试，这里主要讲解静态数据的抓取。 静态数据介绍一下 REST API介绍一下 JSON 文件 获取指定用户发布的信息我们以《人民日报》的推特为例来说明如何抓取指定用户发布的推特信息。 123456789101112import jsonfrom tweepy import Cursor# 设置用户名，这里指的是 id 名，而不是昵称user = 'PDChina'# 将文件存储到以该用户命名的 jsonl 文件中，这里就是 user_timeline_huyong.jsonlfname = "user_timeline_%s.jsonl" % user with open(fname, 'w') as f: for page in Cursor(client.user_timeline, screen_name=user, count=200).pages(16): for status in page: f.write(json.dumps(status._json)+"\n") 抓取到的内容存在一个 jsonl 文件中，因为我们设置了数值，最多 3200 条，也就不是抓取所有的发布。 获取指定用户粉丝和关注的对象信息接下来我们继续爬取《人民日报》的粉丝和关注对象的信息，包括它的简介。为了防止出错，在每个抓取环节前都重新对抓取用户进行赋值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import osimport sysimport jsonimport timeimport mathfrom tweepy import Cursor# 设定了最大值MAX_FRIENDS = 15000max_pages = math.ceil(MAX_FRIENDS / 5000)# 以页数方式爬取def paginate(items, n): for i in range(0, len(items), n): yield items[i:i+n] # 创建用户文件夹，存放该用户相关文件dirname = "users/&#123;&#125;".format(user)try: os.makedirs(dirname, mode=0o755, exist_ok=True)except OSError: print("文件夹 &#123;&#125; 已经存在".format(dirname))except Exception as e: print("在创建文件夹 &#123;&#125; 的时候出错了".format(dirname)) print(e) sys.exit(1) user = 'PDChina' # 获得指定用户的粉丝信息fname = "users/&#123;&#125;/followers.jsonl".format(user)with open(fname, 'w') as f: for followers in Cursor(client.followers_ids, screen_name=user).pages(max_pages): for chunk in paginate(followers, 100): users = client.lookup_users(user_ids=chunk) for user in users: f.write(json.dumps(user._json)+"\n") if len(followers) == 5000: print("正在抓取，休息，休息一下（60s）再继续～") time.sleep(60)user = 'PDChina' # 获取指定用户的关注信息fname = "users/&#123;&#125;/friends.jsonl".format(user)with open(fname, 'w') as f: for friends in Cursor(client.friends_ids, screen_name=user).pages(max_pages): for chunk in paginate(friends, 100): users = client.lookup_users(user_ids=chunk) for user in users: f.write(json.dumps(user._json)+"\n") if len(friends) == 5000: print("正在抓取，休息，休息一下（60s）再继续～") time.sleep(60)user = 'PDChina'# 获取用户信息fname = "users/&#123;&#125;/user_profile.json".format(user)with open(fname, 'w') as f: profile = client.get_user(screen_name=user) f.write(json.dumps(profile._json, indent=4)) 根据抓取量大小情况，抓取时间会有所不同，抓取过程中会出现如下字样，请耐心等待： 正在抓取，休息，休息一下（60s）再继续～ 正在抓取，休息，休息一下（60s）再继续～ 正在抓取，休息，休息一下（60s）再继续～ 程序结束后，可以在程序所在目录中看到多出一个“users”的文件夹，里面会有刚才抓取的“PDChina”，这个文件夹中的三个文件就是刚才获取的： 获取指定内容的搜索信息接下来，我们调用 Twitter 的搜索功能，以“china”为关键词进行检索，抓取 5000 条。这里要注意 Twitter 对搜索进行了限制，只能抓取最近7天内的内容，而不能在任意时间区间内进行检索。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 获取指定内容的搜索信息，并可以设定获取数值# 要检索的内容searchQuery = 'china' # 获取推文的最大值maxTweets = 5000 # 每次请求的最大值tweetsPerQry = 100 fname = "%s_search.jsonl" % searchQuerysinceId = Nonemax_id = -1tweetCount = 0print("最多下载 &#123;0&#125; 条推特".format(maxTweets))with open(fname, 'w') as f: while tweetCount &lt; maxTweets: try: if (max_id &lt;= 0): if (not sinceId): new_tweets = client.search(q=searchQuery, count=tweetsPerQry) else: new_tweets = client.search(q=searchQuery, count=tweetsPerQry, since_id=sinceId) else: if (not sinceId): new_tweets = client.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id - 1)) else: new_tweets = client.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id - 1), since_id=sinceId) if not new_tweets: print("没有发现更多的推特") break for tweet in new_tweets: f.write(json.dumps(tweet._json) +'\n') tweetCount += len(new_tweets) print("已下载 &#123;0&#125; 条推特".format(tweetCount)) max_id = new_tweets[-1].id except tweepy.TweepError as e: print("出错了 : " + str(e)) breakprint ("共下载了 &#123;0&#125; 条推特, 已存入 &#123;1&#125;".format(tweetCount, fname)) 程序开始跑起来的时候，可以看到结果界面不断更新抓取的推特数量，最后我们可以看到总共抓取的推特数量： 最多下载 5000 条推特 已下载 100 条推特 已下载 200 条推特 已下载 300 条推特 已下载 400 条推特 已下载 500 条推特 已下载 600 条推特 已下载 700 条推特 已下载 800 条推特 已下载 898 条推特 已下载 996 条推特 已下载 1096 条推特 已下载 1196 条推特 已下载 1296 条推特 已下载 1387 条推特 已下载 1487 条推特 已下载 1587 条推特 已下载 1687 条推特 已下载 1780 条推特 已下载 1880 条推特 已下载 1980 条推特 已下载 2079 条推特 已下载 2179 条推特 已下载 2279 条推特 已下载 2379 条推特 已下载 2479 条推特 已下载 2535 条推特 已下载 2577 条推特 已下载 2677 条推特 已下载 2777 条推特 已下载 2877 条推特 已下载 2977 条推特 已下载 3074 条推特 已下载 3171 条推特 已下载 3271 条推特 已下载 3367 条推特 已下载 3467 条推特 已下载 3567 条推特 已下载 3667 条推特 已下载 3762 条推特 已下载 3862 条推特 已下载 3962 条推特 已下载 4062 条推特 已下载 4158 条推特 已下载 4258 条推特 已下载 4358 条推特 已下载 4458 条推特 已下载 4558 条推特 已下载 4645 条推特 已下载 4745 条推特 已下载 4845 条推特 已下载 4945 条推特 已下载 5045 条推特 共下载了 5045 条推特, 已存入 china_search.jsonl 动态数据指定标签和内容的信息流因网络环境无法调试，此处省略。 数据处理和分析刚才的抓取获得了三种类型的文件，第一种是用户发布的信息流，我们命名为 user_timelin_PDChina.jsonl ；第二种是用户的粉丝、关注和个人的用户信息，我们把这三个文件放在了一个 “users” 类别的文件夹中，里面还有个以该用户命名的 “PDChina” 文件夹，存放了我们抓取到的 followers.jsonl 、 friends.jsonl 和 user_profile.json 这三个文件；第三种是检索获取的推文信息，我们将其命名为 “china_search.jsonl”。在对数据处理的时候，依据处理的目的，选择不同的文件。利用编辑器可以打开 jsonl 的文件，比如用户发布的信息流文件，打开后我们可以看到密密麻麻的数据，因此，在处理之前我们需要先理解文件内部的结构，这样我们才能清除结构，获取我们需要的信息。 推文的数据结构抓取的信息都存放在了 JavaScript Object Notation (JSON) 文件中，在用编辑器打开的文件中我们可以看到它的主体结构是这样的： 12345678910111213141516&#123; "tweet": &#123; "user": &#123; &#125;, "place": &#123; &#125;, "entities": &#123; &#125;, "extended_entities": &#123; &#125; &#125;&#125; 从这里我们可以发现这个数据结构有一个主对象： tweet（推文信息）；有四个子对象：user（发布者信息），place（地理信息），entities（推文中的非文本信息），extended_entities（推文外的非文本信息）。这五大对象里面还包含各种字段，这些字段有不同的变量类型，可能是数值、字符串或者布尔值等，也可能存放的不只是文本，可以是图片或音频等媒体的链接地址。这样的数据构成可以说就是我们通常所言的“非结构化数据”了，虽然看起来是一种很“结构”的数据。 转发的推文实际上就是在user后面加了一个转推对象 retweeted_status，里面的结构还是推文的结构，也就是一种嵌套了： 123456789101112131415161718192021222324252627282930313233&#123; "tweet": &#123; "user": &#123; &#125;, "retweeted_status": &#123; "tweet": &#123; "user": &#123; &#125;, "place": &#123; &#125;, "entities": &#123; &#125;, "extended_entities": &#123; &#125; &#125;, &#125;, "place": &#123; &#125;, "entities": &#123; &#125;, "extended_entities": &#123; &#125; &#125;&#125; 字段说明字段里面的信息才是我们需要获取的，因此在能清楚数据的结构层次之后，我们应该了解哪些字段的信息是我们需要抓取的。官方文档里提供了五个对象字段（tweet，user，entities， extended entities，geo）的字段介绍，这里主要介绍抓取常常要处理的 Tweet Object 及其子对象 User Object，其他可参阅官方文档。 Tweet object这种数据结构层面的数据和我们直观所见的有很大的不同，数据结构是对存放的信息进行了归类，有些信息并不能体现在我们所见的推文中，但是可见的也是必然有的，不可见的也常常缺失，下面这个是我对官方提供的样例所做的注解： 再对比下面的字段就可以发现，有很多隐性的内容存放在了数据结构之中。 返回字段 字段说明 字段类型 coordinates 地理坐标 Coordinates created_at 推文创建时间 String entities 推文内非文本信息 Entities favorite_count 推文点赞数 Integer favorited 是否认证人点赞了 Boolean geo 地理坐标 Object id 推文唯一识别号 Int64 id_str 推文唯一识别号 String in_reply_to_status_id 这条回复的id Int64 in_reply_to_status_id_str 这条回复的id String in_reply_to_user_id 回复对象的id Int64 in_reply_to_screen_name 回复对象的昵称 String in_reply_to_user_id_str 回复对象的id String is_quote_status 是否为引用的推文 Boolean lang 推文的语言 String place 发推地点 Places possibly_sensitive 敏感内容可能性 Boolean retweet_count 转发数 Int retweeted 是否被认证用户转发 Boolean source 发推设备 String text 推文文本 String truncated 推文是否因超过字数删减 Boolean user 发推者的用户信息 User object quoted_status_id 引用推文的id Int64 quoted_status_id_str 引用推文的 String quoted_status 引用推文区 Tweet retweeted_status 转发推文区 Tweet quote_count 引用推文计数 Integer reply_count 回复计数 Int extended_entities 推文外非文本信息 Extended Entities filter_level 过滤等级 String matching_rules 用于过滤 Array of Rule Objects User object同样在 Tweet 的子对象 User 中，有些字段不是必然有的，有些字段是可变的，因此我们所见的也不完全是数据结构中所含有的。 返回字段 字段说明 字段类型 id 用户的id Int64 id_str 用户的id String name 用户昵称（可改） String screen_name 用户识别名（不可改） String location 用户设定的简介地点 String url 用户提供的主页网址 String description 用户的简介 String derived 用户的一些元数据 Arrays of Enrichment Objects protected 推文是否受保护 Boolean verified 是否为认证用户 Boolean followers_count 粉丝数 Int friends_count 关注数 Int listed_count 该用户属于公开列表的数量 Int favourites_count 用户在使用期间的点赞数 Int statuses_count 用户发推数量，包括转发 Int created_at 用户账号创建时间 String utc_offset The offset from GMT/UTC in seconds Int time_zone 用户设定的时区 String geo_enabled 是否开启地理位置 Boolean lang 用户使用语言 String contributors_enabled 是否开启共同创作者 Boolean profile_background_color 用户使用的背景色 String profile_background_image_url 用户使用的背景图片的地址 String profile_background_image_url_https 用户使用的背景图片的地址 https String profile_background_tile 是否显示背景图片名称 Boolean profile_banner_url 用户横幅网址 String profile_image_url 用户头像网址 String profile_image_url_https 用户头像网址 https String profile_link_color 用户显示链接的颜色 String profile_sidebar_border_color 用户边栏线颜色 String profile_sidebar_fill_color 用户边栏填充颜色 String profile_text_color 用户文本颜色 String profile_use_background_image 用户是否使用背景图片 Boolean default_profile 用户是否切换用户主题 Boolean default_profile_image 用户是否上传了自己的头像 Boolean withheld_in_countries 隐藏地区 String withheld_scope 隐藏范围 String is_translator 用户是否加入译者计划 Boolean 其他对象 Entities object Extend Entities object Geo object 指定字段的抓取方法通过上面的数据收集，我们可以获得五个文件，分别是：指定用户的推文、粉丝信息、关注信息、用户信息和指定内容的检索信息。实际上是两类，一类是推文整体，一类是局部的用户对象（user object）。前面我们已经熟悉了推文的字段，还重点看了下用户的字段，这些字段其实就存在数据结构之中，也就是说信息被结构所掩盖了，我们恰恰需要清除结构，提取我们需要的重要信息，那么，我们是否能有一种方法任意地提取指定的字段信息呢？下面我们以提取 #标签 为例来发展一套普遍的提取方法。 标签处理（hashtags）为获取 #标签（hashtags），我们首先要分析下它存在什么地方。 第一个层级是在 tweet 中的 entities 里： 12345678910"entities": &#123; "hashtags": [ ], "urls": [ ], "user_mentions": [ ], "symbols": [ ] &#125; 第二个层级是在entities中的hashtags里： 1234567891011&#123; "hashtags": [ &#123; "indices": [ 32, 38 ], "text": "nodejs" &#125; ]&#125; 在 hashtags 里有两个字段，但可以有多个存在，也就是说可以有很多个标签，它们都在里面，且各自包含下面这样的字段类型： 返回字段 字段说明 字段类型 indices 索引，第一个数字是这个标签在推特文本的位置，两个数值相减再加上“#”就是字符长度 Array of Int text 标签文本内容 String 这样我们实际需要的就是 text 里面的内容了。 定义获取“标签”字段的方法上面的数据结构实际上就是 Python 中的字典结构，我们就可以利用 dict 中的 get 方法来获取指定字段： 1234def get_hashtags(tweet): entities = tweet.get('entities', &#123;&#125;) hashtags = entities.get('hashtags', []) return [tag['text'].lower() for tag in hashtags] 指定字段提取方法就是仿照上面的代码处理方式，首先明确该字段在数据结构中的位置，然后运用字典的提取方法获取字段的内容。 计算标签频数以一个简单的例子来测试下，将 fname 这个变量设置为获取的指定用户发布信息的文件名，这里是user_timeline_PDChina.jsonl ，然后运行程序获取推文的标签，并计算标签的频数，由多到少排列前20个： 123456789101112131415from collections import Counterfname = 'user_timeline_PDChina.jsonl'with open(fname, 'r') as f:# 引入一个计数的函数 hashtags = Counter() for line in f: tweet = json.loads(line) hashtags_in_tweet = get_hashtags(tweet) hashtags.update(hashtags_in_tweet)# 提取前二十个 for tag, count in hashtags.most_common(20): print("&#123;&#125;: &#123;&#125;".format(tag, count)) 下面就是输出结果，这里提取了《人民日报》推特所发布推文中含有的标签前20名： breaking: 142 dprk: 78 xijinping: 64 china: 60 earthquake: 58 update: 45 19thcpc: 45 brics: 39 peoplesdailycomments: 32 russia: 23 beijing: 23 india: 21 panda: 20 southchinasea: 19 brics2017: 18 beltandroad: 17 shanghai: 15 sichuan: 15 japan: 15 voiceofchina: 14 从话题标签中大致可以看到，《人民日报》在对外宣传中除了突发新闻的报道和领导人的重要新闻外，熊猫也是一张显耀的中国名片。 统计指定发布的标签情况接下来我们统计下《人民日报》发布的推文所带标签的情况： 123456789101112131415161718192021222324252627282930import sysimport jsonfrom collections import defaultdictfname = 'user_timeline_PDChina.jsonl'with open(fname, 'r') as f: hashtag_count = defaultdict(int) for line in f: tweet = json.loads(line) hashtags_in_tweet = get_hashtags(tweet) n_of_hashtags = len(hashtags_in_tweet) hashtag_count[n_of_hashtags] += 1 tweets_with_hashtags = sum([count for n_of_tags, count in hashtag_count.items() if n_of_tags &gt; 0]) tweets_no_hashtags = hashtag_count[0] tweets_total = tweets_no_hashtags + tweets_with_hashtags tweets_with_hashtags_percent = "%.2f" % (tweets_with_hashtags / tweets_total * 100) tweets_no_hashtags_percent = "%.2f" % (tweets_no_hashtags / tweets_total * 100) print("共有 &#123;&#125; 条推文没有话题标签 (占总量的 &#123;&#125;%)".format(tweets_no_hashtags, tweets_no_hashtags_percent)) print("共有 &#123;&#125; 条推文至少有一个话题标签 (占总量的 &#123;&#125;%)".format(tweets_with_hashtags, tweets_with_hashtags_percent)) for tag_count, tweet_count in hashtag_count.items(): if tag_count &gt; 0: percent_total = "%.2f" % (tweet_count / tweets_total * 100) percent_elite = "%.2f" % (tweet_count / tweets_with_hashtags * 100) print("共有 &#123;&#125; 条推文带有 &#123;&#125; 个话题标签 (占总量的 &#123;&#125;% , 占标签推文的 &#123;&#125;% )".format(tweet_count, tag_count, percent_total, percent_elite)) 输出的结果可能是这样： 共有 2014 条推文没有话题标签 (占总量的 62.94%) 共有 1186 条推文至少有一个话题标签 (占总量的 37.06%) 共有 855 条推文带有 1 个话题标签 (占总量的 26.72% , 占标签推文的 72.09% ) 共有 254 条推文带有 2 个话题标签 (占总量的 7.94% , 占标签推文的 21.42% ) 共有 69 条推文带有 3 个话题标签 (占总量的 2.16% , 占标签推文的 5.82% ) 共有 8 条推文带有 4 个话题标签 (占总量的 0.25% , 占标签推文的 0.67% ) 方法举例下面这里提供一些数据处理的方法。 提及处理（user_mentions）“提及”也就是推特或微博中的“@”。 定义获取“提及”字段的方法这个方法也跟标签的处理一样，利用字典的 get 方法抓取指定字段的信息： 1234def get_mentions(tweet): entities = tweet.get('entities', &#123;&#125;) hashtags = entities.get('user_mentions', []) return [tag['screen_name'] for tag in hashtags] 计算提及的频数我们来看下《人民日报》@人的情况，也是获取前20个及计算频数： 1234567891011121314import sysfrom collections import Counterimport jsonfname = 'user_timeline_huyong.jsonl'with open(fname, 'r') as f: users = Counter() for line in f: tweet = json.loads(line) mentions_in_tweet = get_mentions(tweet) users.update(mentions_in_tweet) for user, count in users.most_common(20): print("&#123;&#125;: &#123;&#125;".format(user, count)) 以下是输出结果： realDonaldTrump: 55 UN: 20 Tsinghua_Uni: 8 MedvedevRussiaE: 6 Kunshan_China: 6 WhiteHouse: 5 UNESCO: 5 leehsienloong: 4 EPN: 4 USNavy: 4 narendramodi: 4 MichelTemer: 4 KremlinRussia_E: 4 loveJiangsu: 3 ASEAN: 3 TexasTech: 3 AbeShinzo: 3 PDChina: 3 iaeaorg: 3 NASA: 2 有趣的是第一个竟然是美国总统，而且国外人物或组织较多。 推文文本分析（text）这里的文本分析实际上就是对推文文本进行分词处理，并计算词频。 分词处理和缩写恢复首先我们来设置两个函数，一个是处理分词，一个是处理英文中的缩写。 123456789101112131415161718192021222324from nltk.tokenize import TweetTokenizerfrom nltk.corpus import stopwordsdef process(text, tokenizer=TweetTokenizer(), stopwords=[]): text = text.lower() tokens = tokenizer.tokenize(text) tokens = normalize_contractions(tokens) return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]def normalize_contractions(tokens): token_map = &#123; "i'm": "i am", "you're": "you are", "it's": "it is", "we're": "we are", "we'll": "we will", "china's": "china is", &#125; for tok in tokens: if tok in token_map.keys(): for item in token_map[tok].split(): yield item else: yield tok 计算推文词频在运行这段代码后，可能发现输出结果有较多标点或者缩写没有区分开来，这里就要对上面缩写区分函数和下面代码中处理标点的命令进行修改，添加需要去除的东西，然后我们再运行程序获取前 30 个单词。 12345678910111213141516171819202122import sysimport stringimport jsonfrom collections import Countertweet_tokenizer = TweetTokenizer()punct = list(string.punctuation+'…'+'“'+'’')stopword_list = stopwords.words('english') + punct + ['rt', 'via']fname = 'user_timeline_PDChina.jsonl'tf = Counter()with open(fname, 'r') as f: for line in f: tweet = json.loads(line) tokens = process(text=tweet.get('text', ''), tokenizer=tweet_tokenizer, stopwords=stopword_list) tf.update(tokens) for tag, count in tf.most_common(30): print("&#123;&#125;: &#123;&#125;".format(tag, count)) 以下是输出结果： china: 1354 chinese: 483 us: 227 #breaking: 142 president: 140 new: 136 national: 134 province: 124 first: 113 people: 106 w: 89 says: 89 xi: 85 killed: 81 world: 80 sw: 75 years: 75 injured: 74 city: 71 #dprk: 71 central: 69 police: 67 military: 66 man: 65 south: 63 #xijinping: 63 least: 61 year: 60 #earthquake: 58 beijing: 57 用户粉丝和关注概况12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import sysimport jsonimport numpy as npimport pandas as pdscreen_name = 'PDChina'followers_file = 'users/&#123;&#125;/followers.jsonl'.format(screen_name)friends_file = 'users/&#123;&#125;/friends.jsonl'.format(screen_name)profile_file = 'users/&#123;&#125;/user_profile.json'.format(screen_name)with open(followers_file) as f1, open(friends_file) as f2, open(profile_file) as f: user_profile = json.load(f) followers = [] friends = [] for line in f1: profile = json.loads(line) followers.append(profile['screen_name']) for line in f2: profile = json.loads(line) friends.append(profile['screen_name']) followers = np.array(followers) friends = np.array(friends) mutual_friends = np.intersect1d(friends, followers, assume_unique=True) followers_not_following = np.setdiff1d(followers, friends, assume_unique=True) friends_not_following = np.setdiff1d(friends, followers, assume_unique=True) user_followers = user_profile['followers_count'] user_friends = user_profile['friends_count'] user_name = user_profile['name'] user_statuses = user_profile['statuses_count'] user_time = pd.to_datetime([user_profile['created_at']][0]) user_description = user_profile['description'] print("*****【用户信息摘要】*****") print("----- 用户的实际情况 -----") print("用户 &#123;&#125; 的昵称为：&#123;&#125; ".format(screen_name, user_name)) print("该帐号创建于：&#123;&#125; ".format(user_time)) print("共有 &#123;&#125; 个粉丝 ".format(user_followers)) print("共有 &#123;&#125; 个关注 ".format(user_friends)) print("共发 &#123;&#125; 条推文 ".format(user_statuses)) print('该用户的介绍：&#123;&#125;'.format(user_description)) print("----- 所抓信息的统计结果 -----") print("抓取到该用户的 &#123;&#125; 个粉丝".format( len(followers))) print("抓取到该用户的 &#123;&#125; 个关注".format(len(friends))) print("该用户有 &#123;&#125; 个共同好友".format(len(mutual_friends))) print("其中有 &#123;&#125; 关注对象没有关注该用户 ".format(len(friends_not_following))) print("其中有 &#123;&#125; 粉丝没有被该用户关注".format(len(followers_not_following))) *****【用户信息摘要】***** ----- 用户的实际情况 ----- 用户 PDChina 的昵称为：People's Daily,China 该帐号创建于：2011-05-23 15:00:26 共有 4158120 个粉丝 共有 5331 个关注 共发 55397 条推文 该用户的介绍：The largest newspaper group in China; Timely updates https://t.co/GjIOtXvfA2 https://t.co/nCvfm8gTgr ----- 所抓信息的统计结果 ----- 抓取到该用户的 15000 个粉丝 抓取到该用户的 5331 个关注 该用户有 0 个共同好友 其中有 5331 关注对象没有关注该用户 其中有 15000 粉丝没有被该用户关注 用户影响力比较分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import sysimport jsonscreen_name1 = 'huyong'screen_name2 = 'bbcchinese'followers_file1 = 'users/&#123;&#125;/followers.jsonl'.format(screen_name1)followers_file2 = 'users/&#123;&#125;/followers.jsonl'.format(screen_name2)with open(followers_file1) as f1, open(followers_file2) as f2: reach1 = [] reach2 = [] for line in f1: profile = json.loads(line) reach1.append((profile['screen_name'], profile['followers_count'])) for line in f2: profile = json.loads(line) reach2.append((profile['screen_name'], profile['followers_count']))profile_file1 = 'users/&#123;&#125;/user_profile.json'.format(screen_name1)profile_file2 = 'users/&#123;&#125;/user_profile.json'.format(screen_name2)with open(profile_file1) as f1, open(profile_file2) as f2: profile1 = json.load(f1) profile2 = json.load(f2) followers1 = profile1['followers_count'] followers2 = profile2['followers_count'] tweets1 = profile1['statuses_count'] tweets2 = profile2['statuses_count']sum_reach1 = sum([x[1] for x in reach1])sum_reach2 = sum([x[1] for x in reach2])avg_followers1 = round(sum_reach1 / followers1, 2)avg_followers2 = round(sum_reach2 / followers2, 2)timeline_file1 = 'user_timeline_&#123;&#125;.jsonl'.format(screen_name1)timeline_file2 = 'user_timeline_&#123;&#125;.jsonl'.format(screen_name2)with open(timeline_file1) as f1, open(timeline_file2) as f2: favorite_count1, retweet_count1 = [], [] favorite_count2, retweet_count2 = [], [] for line in f1: tweet = json.loads(line) favorite_count1.append(tweet['favorite_count']) retweet_count1.append(tweet['retweet_count']) for line in f2: tweet = json.loads(line) favorite_count2.append(tweet['favorite_count']) retweet_count2.append(tweet['retweet_count'])avg_favorite1 = round(sum(favorite_count1) / tweets1, 2)avg_favorite2 = round(sum(favorite_count2) / tweets2, 2)avg_retweet1 = round(sum(retweet_count1) / tweets1, 2)avg_retweet2 = round(sum(retweet_count2) / tweets2, 2)favorite_per_user1 = round(sum(favorite_count1) / followers1, 2)favorite_per_user2 = round(sum(favorite_count2) / followers2, 2)retweet_per_user1 = round(sum(retweet_count1) / followers1, 2)retweet_per_user2 = round(sum(retweet_count2) / followers2, 2)print("----- Stats &#123;&#125; -----".format(screen_name1))print("&#123;&#125; followers".format(followers1))print("&#123;&#125; users reached by 1-degree connections".format(sum_reach1))print("Average number of followers for &#123;&#125;'s followers: &#123;&#125;".format(screen_name1, avg_followers1))print("Favorited &#123;&#125; times (&#123;&#125; per tweet, &#123;&#125; per user)".format(sum(favorite_count1), avg_favorite1, favorite_per_user1))print("Retweeted &#123;&#125; times (&#123;&#125; per tweet, &#123;&#125; per user)".format(sum(retweet_count1), avg_retweet1, retweet_per_user1))print("----- Stats &#123;&#125; -----".format(screen_name2))print("&#123;&#125; followers".format(followers2))print("&#123;&#125; users reached by 1-degree connections".format(sum_reach2))print("Average number of followers for &#123;&#125;'s followers: &#123;&#125;".format(screen_name2, avg_followers2))print("Favorited &#123;&#125; times (&#123;&#125; per tweet, &#123;&#125; per user)".format(sum(favorite_count2), avg_favorite2, favorite_per_user2))print("Retweeted &#123;&#125; times (&#123;&#125; per tweet, &#123;&#125; per user)".format(sum(retweet_count2), avg_retweet2, retweet_per_user2)) 【插入结果】 用户粉丝分析（description）12345678910111213141516171819202122232425262728293031323334353637383940import sysimport jsonfrom argparse import ArgumentParserfrom collections import defaultdictfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import KMeansfname = 'users/huyong/followers.jsonl' # 设定文件路径if min_ngram &gt; max_ngram: print("Error: incorrect value for --min-ngram (&#123;&#125;): it can't be higher than --max-value (&#123;&#125;)".format(min_ngram, max_ngram)) sys.exit(1)with open(fname) as f: # load data users = [] for line in f: profile = json.loads(line) users.append(profile['description']) # create vectorizer vectorizer = TfidfVectorizer(max_df= 0.8, min_df= 2, max_features= 200, stop_words='english', ngram_range=(1, 3), use_idf= True) # fit data X = vectorizer.fit_transform(users) print("Data dimensions: &#123;&#125;".format(X.shape)) # perform clustering km = KMeans(n_clusters= 5) km.fit(X) clusters = defaultdict(list) for i, label in enumerate(km.labels_): clusters[label].append(users[i]) # print 10 user description for this cluster for label, descriptions in clusters.items(): print('---------- Cluster &#123;&#125;'.format(label)) for desc in descriptions[:10]: print(desc) 【插入结果】 用户回应关系分析1234567891011121314151617181920212223242526272829303132333435363738import sysimport jsonfrom operator import itemgetterimport networkx as nxfname = 'CPC china_search.jsonl'with open(fname) as f: graph = nx.DiGraph() #创建有向图 for line in f: tweet = json.loads(line) if 'id' in tweet: # 添加节点， graph.add_node(tweet['user']['id'], tweet=tweet['text'], author=tweet['user']['screen_name'], created_at=tweet['created_at']) if tweet['in_reply_to_user_id']: reply_to = tweet['in_reply_to_user_id'] if tweet['in_reply_to_user_id'] in graph \ and tweet['user']['screen_name'] != graph.node[reply_to]['author']: graph.add_edge(tweet['in_reply_to_user_id'], tweet['user']['id']) print(nx.info(graph)) sorted_replied = sorted(graph.degree_iter(), key=itemgetter(1), reverse=True) most_replied_id, replies = sorted_replied[0] print("Most replied tweet (&#123;&#125; replies):".format(replies)) print(graph.node[most_replied_id]) print("Longest discussion:") longest_path = nx.dag_longest_path(graph) for tweet_id in longest_path: node = graph.node[tweet_id] print("&#123;&#125; (by &#123;&#125; at &#123;&#125;)".format(node['tweet'], node['author'], node['created_at'])) 【插入结果】 数据的可视化推文发布时间序列分析（created_at）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import sysimport jsonfrom datetime import datetimeimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport pandas as pdimport numpy as npimport picklefname = 'user_timeline_PDChina.jsonl'# 打开指定文件，获取时间字段，将时间索引与计数对应with open(fname, 'r') as f: all_dates = [] for line in f: tweet = json.loads(line) #tweet.pop('retweeted_status', &#123;&#125;) all_dates.append(tweet.get('created_at')) # 获取时间标签 ones = np.ones(len(all_dates)) # 有点像在做计数赋值1 idx = pd.DatetimeIndex(all_dates) # 把时间字符转换为真正的时间变量 my_series = pd.Series(ones, index=idx) # 把时间和计数对应上 per_day = my_series.resample('D', how='sum').fillna(0) # ‘M’ 表示月 ‘D’ 表示日 print(my_series.head()) print(per_day.head()) fig, ax = plt.subplots() ax.grid(True) ax.set_title("Tweet Frequencies") #days = mdates.MonthLocator(interval=17) # 月份标记，可以控制月份间隔 days = mdates.DayLocator(interval=20) date_formatter = mdates.DateFormatter('%Y-%m-%d') # 要注意了，M 是分钟，m 才是月，D 是整个日期，d 是日 # 最小时间到最大时间 datemin = datetime(2009, 1, 1) datemax = datetime(2017, 11, 1) ax.xaxis.set_major_locator(days) ax.xaxis.set_major_formatter(date_formatter) ax.set_xlim(datemin, datemax, auto=True) max_freq = per_day.max() ax.set_ylim(0, max_freq) ax.plot(per_day.index, per_day) plt.show() 数据的摘要： 2017-11-05 07:01:41 1.0 2017-11-05 06:35:01 1.0 2017-11-05 06:10:02 1.0 2017-11-05 05:50:02 1.0 2017-11-05 05:24:57 1.0 dtype: float64 2017-08-05 26.0 2017-08-06 30.0 2017-08-07 37.0 2017-08-08 41.0 2017-08-09 40.0 Freq: D, dtype: float64 所抓取推特中《人民日报》的发推时间序列： 1.关于OAuth 可参考：理解 OAuth 2.0 。 ↩2.Twitter 的这两种 API 的认证模式可以参考官方介绍：Twitter API Authentication Model。 ↩3.参见 How to use Twitter’s Search REST API most effectively。 ↩4.可以参考官方文档 Rate limits per window。 ↩]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么理解 Python 爬虫]]></title>
    <url>%2F2017%2F10%2F20%2F2017-10-20%2F</url>
    <content type="text"><![CDATA[由于自己的研究需要爬取 Twitter 和 Facebook 的社交数据，但自己又没有真正爬取过，那么我就来好好学习一下啦。 选择一本书我选择了《Mastering social media mining with Python》这本书，里面的讲解还是很直接的，干货多方便我快速掌握。本书的代码都能在网站上获取 了解一些理论第一章是 Social media - challenges and opportunities ，讲的是一些爬虫的基本知识，还有一些技术展示，重在培养对爬虫的兴趣。 对社交媒体的理解Internet-based applicationsUser-generated contentNetworking 本书要解决的问题how to extract useful knowledge from the data coming from the social media? knowledge hierarchy 社交媒体挖掘的机遇Application Programming Interface (API)Representational State Transfer (REST)RESTful API 社交媒体挖掘的挑战big datastructured dataunstructured data semi-structured data data integrity data access research and development (R&amp;D) processes 社交媒体挖掘的技术流程1、验证 Authentication2、数据收集 Data collection3、数据清理和预处理 Data cleaning and pre-processing4、建模和分析 Modeling and analysis5、结果展示 Result presentation Open Authorization (OAuth)a user, consumer (our application), and resource provider (the social media platform). Text miningGraph mining Python tools for data sciencePython 的优点Declarative and intuitive syntaxRich ecosystem for data processingEfficiency Python 版本3.4+ and 3.5+pip and virtualenvvirtualenv 管理虚拟环境和安装依赖环境Conda, Anaconda, and Miniconda 数据分析工具NumPy and pandasNaming conventionsnumpy as np; pandas as pd; 机器学习Supervised learningNaive Bayes (NB)Support Vector Machine (SVM)Neural Networks (NN)training datatest dataUnsupervised learning]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终于弄好了～]]></title>
    <url>%2F2017%2F10%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[花了一点时间弄这个，可以建一个自己的博客也是很好了。]]></content>
      <categories>
        <category>生活日志</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
